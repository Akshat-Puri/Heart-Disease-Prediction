# -*- coding: utf-8 -*-
"""Heart_Disease_Prediction.ipynb

Automatically generated by Colaboratory.

# **Heart Disease Prediction Using Machine Learning**
Prediction and diagnosing heart disease is the biggest challenge in the medical industry and relies on the 
factors such as the physical examination, symptoms and signs of the patient. The correct prediction of heart 
disease can prevent life threats, and incorrect prediction can prove to be fatal at the same time. In this 
project different machine learning algorithms and deep learning are applied to compare the results and 
analysis of the heart disease dataset to train a model for the task of heart disease prediction using Machine 
Learning.
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Necessary Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf

#import cufflinks as cf
# %matplotlib inline

# Metrics for Classification technique
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

# Scaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import  RandomizedSearchCV, train_test_split

!pip install catboost

from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# Importing Data from the Device
from google.colab import files

uploaded = files.upload()

import io

data = pd.read_csv(io.BytesIO(uploaded['heart.csv']))
data.head(6) # Mention no of rows to be displayed from the top in the argument

"""**Exploratory Data Analysis**"""

# Size of the dataset
data.shape

"""We have a dataset with 303 rows which indicates a smaller set of data."""

data.info()

"""

*   Out of 14 features, we have 13 int type and only one with float data type.
*   Woah! We have no missing values in our dataset.

"""

data.describe()

"""Let's check correleation between various features."""

plt.figure(figsize=(20,12))
sns.set_context('notebook',font_scale = 1.3)
sns.heatmap(data.corr(),annot=True,linewidth =2)
plt.tight_layout()

"""Let's check the correlation of various features with the target feature."""

sns.set_context('notebook',font_scale = 2.3)
data.drop('target', axis=1).corrwith(data.target).plot(kind='bar', grid=True, figsize=(20, 10), 
                                                        title="Correlation with the target feature")
plt.tight_layout()

"""*   Four feature( "cp", "restecg", "thalach", "slope" ) are positively correlated with the target feature.
*   Other features are negatively correlated with the target feature.

Individual Feature Analysis

**Age("age") Analysis**
"""

# Let's check 10 ages and their count
plt.figure(figsize=(25,12))
sns.set_context('notebook',font_scale = 1.5)
sns.barplot(x=data.age.value_counts()[:10].index,y=data.age.value_counts()[:10].values)
plt.tight_layout()

"""Let's check the range of age in the dataset."""

minAge=min(data.age)
maxAge=max(data.age)
meanAge=data.age.mean()
print('Min Age :',minAge)
print('Max Age :',maxAge)
print('Mean Age :',meanAge)

"""We should divide the Age feature into three parts - "Young", "Middle" and "Elder""""

Young = data[(data.age>=29)&(data.age<40)]
Middle = data[(data.age>=40)&(data.age<55)]
Elder = data[(data.age>55)]

plt.figure(figsize=(23,10))
sns.set_context('notebook',font_scale = 1.5)
sns.barplot(x=['young ages','middle ages','elderly ages'],y=[len(Young),len(Middle),len(Elder)])
plt.tight_layout()

"""A large proportion of dataset contains Elder people.

Elderly people are more likely to suffer from heart disease.
"""

colors = ['blue','green','yellow']
explode = [0,0,0.1]
plt.figure(figsize=(10,10))
sns.set_context('notebook',font_scale = 1.2)
plt.pie([len(Young),len(Middle),len(Elder)],labels=['young ages','middle ages','elderly ages'],explode=explode,colors=colors, autopct='%1.1f%%')
plt.tight_layout()

"""**Sex("sex") Feature Analysis**"""

plt.figure(figsize=(18,9))
sns.set_context('notebook',font_scale = 1.5)
sns.countplot(data['sex'])
plt.tight_layout()

"""Ratio of Male to Female is approx 2:1"""

# Let's plot the relation between sex and slope.
plt.figure(figsize=(18,9))
sns.set_context('notebook',font_scale = 1.5)
sns.countplot(data['sex'],hue=data["slope"])
plt.tight_layout()

plt.figure(figsize=(18,9))
sns.set_context('notebook',font_scale = 1.5)
sns.countplot(data['cp'])
plt.tight_layout()

"""1. status at least
2. condition slightly distressed
3. condition medium problem
4. condition too bad
"""

plt.figure(figsize=(18,9))
sns.set_context('notebook',font_scale = 1.5)
sns.countplot(data['thal'])
plt.tight_layout()

plt.figure(figsize=(18,9))
sns.set_context('notebook',font_scale = 1.5)
sns.countplot(data['target'])
plt.tight_layout()

categorical_val = []
continous_val = []
for column in data.columns:
    print("--------------------")
    print(f"{column} : {data[column].unique()}")
    if len(data[column].unique()) <= 10:
        categorical_val.append(column)
    else:
        continous_val.append(column)

"""Now here first we will be removing the target column from our set of features then we will categorize all the categorical variables using the get dummies method which will create a separate column for each category suppose X variable contains 2 types of unique values then it will create 2 different columns for the X variable."""

categorical_val.remove('target')
dfs = pd.get_dummies(data, columns = categorical_val)
dfs.head(6)

"""Now we will be using the standard scaler method to scale down the data so that it wonâ€™t raise the outliers also dataset which is scaled to general units leads to having better accuracy."""

sc = StandardScaler()
col_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
dfs[col_to_scale] = sc.fit_transform(dfs[col_to_scale])
dfs.head(6)

#Modeling: splitting our dataset
X = dfs.drop('target', axis=1)
y = dfs.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#K-Nearest Neighbor Machine Learning Algorithm
knn = KNeighborsClassifier(n_neighbors = 10)
knn.fit(X_train,y_train)
y_pred1 = knn.predict(X_test)
print(accuracy_score(y_test,y_pred1))

# Hyperparameter Optimization

test_score = []
neighbors = range(1, 25)

for k in neighbors:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    test_score.append(accuracy_score(y_test, model.predict(X_test)))

plt.figure(figsize=(18, 8))
plt.plot(neighbors, test_score, label="Test score")
plt.xticks(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()
plt.tight_layout()

#K-Nearest Neighbor Machine Learning Algorithm
knn = KNeighborsClassifier(n_neighbors = 19)
knn.fit(X_train,y_train)
y_pred1 = knn.predict(X_test)
print(accuracy_score(y_test,y_pred1))

test_score = accuracy_score(y_test, knn.predict(X_test)) * 100
train_score = accuracy_score(y_pred1, knn.predict(X_test)) * 100

results_df = pd.DataFrame(data=[["K-Nearest Neighbors", train_score, test_score]], 
                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])
results_df
